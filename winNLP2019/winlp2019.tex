%
% File winlp2019.tex is modified based on File coling2018.tex
% 
% Contact: winlp-chairs@googlegroups.com
% if they can't help, contact: zhu2048@gmail.com & liuzy@tsinghua.edu.cn
%% Based on the style files for COLING-2018, which were in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2018}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{cleveref}



%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Controlling the Specificity of Clarification Question Generation}


\author{Trista Yang Cao \\
University of Maryland \\
{\tt ycao95@cs.umd.edu} \\ \And
Sudha Rao\thanks{This research was performed when the author was still at University of Maryland, College Park.} \\
Microsoft Research, Redmond \\
{\tt Sudha.Rao@microsoft.com} \\ \And
Hal Daum\'e III \\
University of Maryland \\
Microsoft Research, New York City\\
{\tt me@hal3.name} }


\date{}

\begin{document}
\maketitle
\begin{abstract}
Unlike comprehension-style questions, clarification questions look for some missing information in a given context. 
However, without guidance, neural models for question generation, just like other text generation tasks, lead to generic and bland questions that cannot elicit useful information. 
We propose a neural clarification question generation model with controllable level of specificity to the given context. 
To train the question generation model, we automatically generate the specificity labels (generic or specific) using a classifier trained on a small amount of annotated data. 
Evaluated on Amazon questions dataset, our model demonstrates controllable specificity according to both automatic metrics and human judgments.

\end{abstract}


\section{Introduction}
\label{intro}
Recent advances in neural network modeling have triggered several sequence-to-sequence learning \cite{sutskever2014sequence} based methods for question generation.
Serban et al. \shortcite{serban2016generating} created a large (30 million) factoid question-answering dataset  by transforming facts in the Freebase into natural language questions. 
Their question generation model is inspired by the well-known attention-based encoder-decoder model \cite{luong2015effective} used for machine translation.
Duan et al. \shortcite{duan2017question} extract a large number of question-answer pairs from community question answering forums and use them for training an attention-based sequence-to-sequence learning approach to generate challenging questions for the reading comprehension task.  
Du et al. \shortcite{du2017learning} propose an attention-based encoder-decoder model for generating questions from text passages and show that humans find the questions generated by their model to be more natural and more challenging to answer compared to rule-based systems.  

We find that training a sequence-to-sequence neural network model to generate a clarification question given a context results in over-generic questions, similar to recent findings in dialogue generation \cite{li2016deep}. 
In our task, we define generic questions to be questions that can apply to almost all items, whereas specific questions are the ones can be only applied to the specific item or other very similar items. 
For instance, in figure \ref{amazon-ex-1}, given a product and its description, there are one generic, and one specific question asked about this product. 

However, the problem of concretely measuring the specificity level of text has received sparse attention. 
Louis and Nenkova  \shortcite{louis2011automatic} first introduce a supervised binary classifier to identify whether a summary of the given context is specific or generic. 
Gao et al. \shortcite{Gao2019PredictingAA} then propose a supervised regression model that evaluates the level of specificity of sentences. 
While those works are focusing on identifying the specificity level of text, our task hopes to use the classifier as guidance to control the level of specificity of the generated questions. 

To achieve this, we take a semi-supervised approach to our problem of generating specificity controlled questions. 
For training data, we first construct a model that automatically predict question's specificity level using a small amount of annotated data (\cref{classifier}).  
Then motivated by Sennrich et al. \cite{sennrich2016controlling}, we build a question generation model that incorporates the level of specificity as an additional input signal during training\footnote{Sennrich et al. \cite{sennrich2016controlling} refer to this as side constraints.} (\cref{model}). 
During test time, given a new context and a level of specificity (which is either generic or specific), the model then generates question at that level of specificity.

\begin{figure*}[h]
	\includegraphics[width=\textwidth]{amaz-ex.png}
    \caption{Sample product description from amazon.com paired with a generic and a specific clarification question.}\label{amazon-ex-1}
\end{figure*}



\section{Model for Automatically Predicting Specificity Level}\label{classifier}

Given the specific/generic annotations on a subset of our training data, we want to train a machine learning model that can learn to predict the specificity level given a context and a question. We use some of the features described in Louis and Nenkova's work \cite{louis2011automatic} and introduce some new features relevant to our setting to create a similar classifier that predicts the level of specificity of a question given its context. Based on these features, we train a logistic regression model to make a binary prediction (-1: generic, 1: specific) given a context and a question. We use the Support Vector Regression (SVR) model with Radial Basis Function (RBF) kernel. Gao et al. \shortcite{Gao2019PredictingAA}, in their work of analyzing language in social media post, claim SVR with RBF has the best performance in predicting text specificity.\\

%\begin{figure*}[h]
%\centering
%	\includegraphics[scale = 0.3]{roadmap.png}
%    \caption{The behaviour of our model during test time.}\label{roadmap}
%\end{figure*}

\section{Specificity-Controlled Question Generation Model}\label{model}
The key idea behind those sequence-to-sequence approaches is that given large amounts of input, output sequence pairs, the model learns internal representations such that at test time, given an input sequence, it generates the appropriate output sequence. 
We use the specificity classifier described in the previous section to label all the questions in the training (and tune) data with generic/specific labels. 
We use these labels to append each context with the \textit{$<$specific$>$} tag when the question paired with the context is labeled as specific and with the \textit{$<$generic$>$} tag when the question paired with the context is labeled as generic.
We train a sequence-to-sequence learning model \cite{sutskever2014sequence} on (context+specificity, question) pairs using maximum likelihood objective. 
At test time, given a new context and a desirable level of specificity, we generate a question at that level of specificity.


\section{Results/Conclusion}
We train a specificity classifier which given a context and a question can predict the level of specificity of the question to the context with $73 \%$ accuracy. 
We also conduct experiments for our specificity-controlled question generation model using Amazon questions dataset. We evaluate based on both automatic metrics and human evaluations. Evaluations are still in progress.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{coling2018}

\bibliography{specificity_controlled_clarification}
\bibliographystyle{acl}

\end{document}
